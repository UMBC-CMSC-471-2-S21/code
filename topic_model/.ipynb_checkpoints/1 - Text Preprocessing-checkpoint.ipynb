{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will look at the steps involved in preprocessing a corpus of unstructed text documents using *scikit-learn*, which we will use later for topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our sample corpus of text, we will use a corpus of news articles collected in 2016. These articles have been stored in a single file and formatted so that one article appears on each line. We will load these articles into a list, and also create a short snippet of text for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4551 raw text documents\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "raw_documents = []\n",
    "snippets = []\n",
    "with open( os.path.join(\"data\", \"articles.txt\") ,\"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        raw_documents.append( text )\n",
    "        # keep a short snippet of up to 100 characters as a title for each article\n",
    "        snippets.append( text[0:min(len(text),100)] )\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barclays' defiance of US fines has merit Barclays disgraced itself in many ways during the pre-financial crisis boom years. So it is tempting to think the bank, when asked by US Department of Justice to pay a large bill for polluting the financial system with mortgage junk between 2005 and 2007, should cough up, apologise and learn some humility. That is not the view of the chief executive, Jes Staley. Barclays thinks the DoJ’s claims are “disconnected from the facts” and that it has “an obligation to our shareholders, customers, clients and employees to defend ourselves against unreasonable allegations and demands.” The stance is possibly foolhardy, since going into open legal battle with the most powerful US prosecutor is risky, especially if you end up losing. But actually, some grudging respect for Staley and Barclays is in order. The US system for dishing out fines to errant banks for their mortgage sins has come to resemble a casino. The approach prefers settlements behind closed doors and the difference in size of penalties is never explained. Occasional leaks of the negotiating demands make the methodology appear even more arbitrary. Deutsche Bank was initially asked for $14bn (£11.5bn), but reached a settlement of $7.2bn on Thursday. Where is the rhyme or reason? There is also a strong suspicion that the roulette wheel is weighted against the Europeans. US banks, in the forms of JP Morgan, Goldman Sachs, Morgan Stanley, Bank of America and Citi, were at the front of the queue for settlement for no obvious reason. If Barclays created and distributed far fewer toxic mortgage securities than its US rivals, which is what the bank argues, why shouldn’t its fine be proportionately smaller? Neither Barclays nor the DoJ is talking hard numbers. But Barclays, it is said, was asked for $4bn, versus its own analysis that a fair sum would be $1bn and $2bn could have been swallowed for the sake of certainty. When the gap is so wide, Barclays is entitled to take its chances in court – and yes, it probably has an obligation to do so. A board can’t let $2bn slip out of the door just for the sake of a quiet life. The case will be messy, inevitably. Barclays’ practices were “plainly irresponsible and dishonest,” according to Loretta Lynch, the US attorney general. There is also a cache of ugly emails and documents. The DoJ lawsuit says Barclays employees called one parcel of securitised loans “craptacular”. Another was said to “look like shit”. However, that is almost par for the course in these cases. The central question is the right size of penalty. If Barclays thinks it has been singled out for unduly harsh treatment, the bank should try to prove its case. Staley will look like a fool if he fails, but the willingness to reject the easy option of settling is entirely legitimate.\n"
     ]
    }
   ],
   "source": [
    "print(raw_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preprocessing text, a common approach is to remove non-informative stopwords. The choice of stopwords can have a considerable impact later on. We will use a custom stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword list has 350 entries\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append( line.strip() )\n",
    "# note that we need to make it hashable\n",
    "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'according', 'across']\n"
     ]
    }
   ],
   "source": [
    "print(custom_stop_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *bag-of-words model*, each document is represented by a vector in a *m*-dimensional coordinate space, where *m* is number of unique terms across all documents. This set of terms is called the corpus *vocabulary*. \n",
    "\n",
    "Since each document can be represented as a term vector, we can stack these vectors to create a full *document-term matrix*. We can easily create this matrix from a list of document strings using *CountVectorizer* from Scikit-learn. The parameters passed to *CountVectorizer* control the pre-processing steps that it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4551 X 6263 document-term matrix\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# use a custom stopwords list, set the minimum term-document frequency to 40\n",
    "vectorizer = CountVectorizer(stop_words = custom_stop_words, min_df = 40)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6263)\n",
      "['innovative', 'inquiry', 'inside', 'insiders', 'insight', 'insist', 'insisted', 'insistence', 'insisting', 'insists', 'inspiration', 'inspire', 'inspired', 'inspiring', 'instagram', 'installed', 'instance', 'instant', 'instantly', 'instinct']\n"
     ]
    }
   ],
   "source": [
    "print(A[0,:].shape)\n",
    "print(vectorizer.get_feature_names()[3000:3020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process also builds a vocabulary for the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 6263 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['innovative', 'inquiry', 'inside', 'insiders', 'insight', 'insist', 'insisted', 'insistence', 'insisting', 'insists', 'inspiration', 'inspire', 'inspired', 'inspiring', 'instagram', 'installed', 'instance', 'instant', 'instantly', 'instinct']\n"
     ]
    }
   ],
   "source": [
    "print(terms[3000:3020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this document-term matrix, terms, and snippets for later use using *Joblib* to persist the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-raw.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump((A,terms,snippets), \"articles-raw.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Term Weighting with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the usefulness of the document-term matrix by giving more weight to the more \"important\" terms. The most common normalisation is *term frequency–inverse document frequency* (TF-IDF). In Scikit-learn, we can generate at TF-IDF weighted document-term matrix by using *TfidfVectorizer* in place of *CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4551 X 6263 TF-IDF-normalized document-term matrix\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, min_df = 40)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 6263 distinct terms\n"
     ]
    }
   ],
   "source": [
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '100m', '10m']\n"
     ]
    }
   ],
   "source": [
    "print(terms[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple characterisation that we might do would be to look at the terms with the highest TF-IDF scores across all documents in the document-term matrix. We can define such a function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now display a ranking of the top 20 terms, which gives us a very rough sense of the content of the document collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. trump (203.60)\n",
      "02. people (119.21)\n",
      "03. eu (115.55)\n",
      "04. film (102.55)\n",
      "05. uk (95.48)\n",
      "06. bank (84.66)\n",
      "07. time (83.66)\n",
      "08. brexit (72.01)\n",
      "09. health (66.87)\n",
      "10. back (66.38)\n",
      "11. government (64.89)\n",
      "12. clinton (63.29)\n",
      "13. get (62.84)\n",
      "14. world (62.74)\n",
      "15. women (60.68)\n",
      "16. way (60.33)\n",
      "17. campaign (60.13)\n",
      "18. before (60.04)\n",
      "19. work (59.36)\n",
      "20. vote (57.47)\n"
     ]
    }
   ],
   "source": [
    "ranking = rank_terms( A, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will save this document-term matrix, terms, and snippets for topic modelling later using *Joblib*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-tfidf.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((A,terms,snippets), \"articles-tfidf.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
